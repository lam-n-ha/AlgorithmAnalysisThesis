\chapter{Code Components}\label{chapter:code}

% To perform algorithm analysis with JuPE, the user needs to follow the folliowing 3 steps:
% \begin{enumerate}
% 	\item Choose from a supported list the class of function to be optimized.
% 	\item Define an algorithm to be analyzed .
% 	\item Specify a performance measure.
%   \end{enumerate}
 
% \begin{figure}[hbtp]
% 	\begin{lstlisting}[mathescape]
% m,L = 1,10
% $ \alpha $ = 2/(L+m)
% @algorithm begin

% 	f = DifferentiableFunctional{R$ ^n $}()
% 	xs = first_order_stationary_point(f)
% 	f' $ \in $ SectorBounded(m, L, xs, f'(xs))

% 	x0 = R$ ^n $()
% 	x1 = x0 - $ \alpha $*f'(x0)

% 	x0 => x1

% 	performance = (x1-xs)^2
% end

% @show rate(performance)
% \end{lstlisting}
% \caption{Analysis Example
% \label{ex_analysis}
% \end{figure}


% In the example code, the user:
% \begin{itemize}
% 	\item Define the class of function f \comment{(use $f$ for mathematical symbols, or \texttt{f} for programming symbols)} and its gradient f' by calling one of the provided functions. In this example, the class of function is 1 - 10 sector bounded functions. \comment{Need to define what this means.}
% 	\item Set the global minimum goal as a stationary point $ x_s $\comment{.}
% 	\item Define an initial state $ x_0 $ and the algorithm with which its next state $ x_1$ is updated. In this example, the algorithm being analyzed is gradient descent with a step size $ \alpha $= 2/11 \comment{(include all math, such as $=2/11$, in dollar signs)}.
% 	\item Set the performance measure as the norm distance between the initial state and the goal $ (x_0 - x_s)^2 $. The returned convergence rate guarantee is the rate at which the performance measure is decreasing after each iteration of the algorithm.
% 	\item Call the rate function \comment{to} perform automated algorithm analysis.
% \end{itemize}

% With the calling of the rate function, JuPE derives every necessary input from the performance measure and performs analysis to return a rate of 0.6687164306640625. This means for the provided algorithm and every function in the class, the convergence rate of the performance measure is guaranteed to match or exceed the result worst-case guarantee rate. \comment{Need to describe what exactly is meant by the convergence rate. In particular, the analysis returns the scalar $\rho$ such that, for some constant $c>0$, the performance measure is upper bounded by $c \rho^k$ at each iteration $k$. If you would like to use ``big-oh'' notation, this means that the performance measure converges with rate $O(\rho^k)$.}
 
% \begin{figure}[hbtp]
% \begin{lstlisting}
% rate(performance) = 0.6687164306640625
% \end{lstlisting}
% \caption{Analysis result}
% \label{ex_result}
% \end{figure}

Algorithm Analysis derive a guaranteed worst-case convergence rate by following the set of instructions presented in section 3 to create and solve an optimization problem and derive a performance certification. Implenmenting a mathematical procedure as code presents a list of challenges which includes being able to understand and differentiate between variables, represent concepts such as gradients or states, and formulating and solving a convex optimization problem, while keeping the users' interaction with the program simple. This chapter goes into the code that constitutes Algorithm Analysis and enables these functionalities.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Expressions}

Expressions are data structures used to represent mathematical variables and enable the implementation of the concepts presented in \Cref{chapter:lyapunov} into a computer program. Expressions can either be vectors or scalar in an inner product space, making them the smallest building block with which concepts such as gradient, constraints, or states are built.

\subsection*{Variable and Decomposition}
Expressions can either be a variable expression or a decomposition expression representing some combination of variable expressions. An expression's decomposition is stored in its \texttt{value} field.
\begin{description}
	\item[Variable expression] is defined to be in a field and represents either a vector or scalar. Its decomposition is itself.
\begin{figure}[h]
		\begin{lstlisting}[mathescape]
@algorithm begin
	x0 = R$^n$()
	y0 = R$^n$()^2
end
@show(x0)
x0 = x0

Vector in R$^n$()
Label: x0
Associations: Dual => x0$^*$

@show(y0)
y0 = y0

Scalar in R$^n$()
Label: y0
Oracles: LinearFunctional{R$^n$}
		\end{lstlisting}
	
	\caption{Example of a variable expression representing a vector}
	\label{ex_variable}
\end{figure}
	\item[Decomposition expression] represents scalars and is some linear combination of scalar variable or decomposition expressions. Its decomposition is a dictionary containing how many of each expression that form the decomposition expression.

\begin{figure}[h]
		\begin{lstlisting}[mathescape]
@show(x0 + 2xs)
x0 + 2xs = 2 xs + x0

Vector in R
	Decomposition: x0 + 2 xs
	Associations: Dual => LinearFunctional{R$^n$}
		\end{lstlisting}
	\caption{Example of a decomposition expression}
	\label{ex_decomposition}
\end{figure}
\end{description}

\section{Algebra}
Expressions in the package belong in an inner product space, which is a set of elements that can be vectors or scalar and which supports certain operations such as norm and inner product in addition to algebraic operations. In Figure~\ref{ex_variable}, expressions are defined to be in R$^n$, an inner product space pre-defined in the package which encompasses n-dimensional real numbers.

Algorithm Analysis supports operations that characterize inner product spaces between expressions. The examples that demonstrate these operations use vector expressions \texttt{x0} and \texttt{y0} in Figure~\ref{ex_variable}.

\subsection*{Addition or subtraction between expressions}

Vectors and numbers can be added together in an inner product space. In an addition operation, if both expressions of the operation posess a 'value', they are added to create the value of a new resulting expression. Otherwise, the result is a new expression whose decomposition is the merging of the decompositions of the expressions in the operation.
\begin{figure}[!h]
	\begin{lstlisting}[mathescape]
a = x0+y0-x0-x0
@show(a)
a = a

Vector in R$^n$
	Label: a
	Decomposition: y0 - x0
	Associations: Dual => a$^*$
	\end{lstlisting}
	\caption{Example of addition and subtraction operation}
	\label{ex_addsub}
\end{figure}

\subsection*{Multiplication or division between an expression and a scalar} 
Vectors and scalars can be scaled in an inner product space. Tbe Algorithm Analysis package performs the multiplication or division of an expression by scaling the value or decomposition of an expression.
\begin{figure}[!h]
	\begin{lstlisting}[mathescape]
c = x0*-3 + y0*2
@show(c)
c = c

Vector in R$^n$
	Label: c
	Decomposition: 2 y0 - 3 x0
	Associations: Dual => c$^*$

	\end{lstlisting}
	\caption{Example of multiplication operation}
	\label{ex_scalar}
\end{figure}

\subsection*{Transpose of a vector}
In the Algorithm Analysis program, the transpose of a vector expression can be taken to create a new expression. When a vector expression is created, a data structure that maps it to its transpose is stored in the \texttt{associations} field, allowing the program to keep track of whether an expression is the tranpose of another.

\begin{figure}[!h]
	\begin{lstlisting}[mathescape]
@show(x0')
x0' = x0$^*$

Oracle
	Description: Linear functional on R$^n$
	Label: x0*
	Properties: Linear()

@show(x0'')
(x0')' = x0

Vector in R$^n$
	Label: x0
	Associations: Dual => x0$^*$
	\end{lstlisting}
	\caption{Example of transpose operation}
	\label{ex_transpose}
\end{figure}

\subsection*{Inner product operation between two vectors}
In an inner product space, the inner product operation of two vectors is possible and result in a scalar. For example, the inner product of vectors y0 and x0, denoted $\innerproduct{y0}{x0}$, is calculated as $x0^\tp * y0$. The Algorithm Analysis package finds the inner product by creating a new vector variable expression with the appropriate label indicating the new expression as an inner product between two other vectors.
\begin{figure}[!h]
	\begin{lstlisting}[mathescape]
inner = x0'*y0
@show(inner)
inner = <y0,x0>

Scalar in R
	Label: <y0,x0>
	Oracles: x0$^*$

	\end{lstlisting}
	\caption{Example of an inner product between two vector expressions}
	\label{ex_inner}
\end{figure}

\subsection*{Squared norm}
An expression of the normed vector vpace type can be squared to produce a an inner product space expression.
\begin{figure}[!h]
	\begin{lstlisting}[mathescape]
norm = x0^2

@show(norm)
norm = |x0|$^2$
Scalar in R
	Label: |x0|$^2$
	Oracles: x0$^*$
\end{lstlisting}
\caption{Example of norm of vector expression}
\label{ex_norm}
\end{figure}

\section{Oracles} \label{oracles}
As mentioned in section 3.2, algorithm analysis relies on interpolation conditions - constraints on the input $y$ and output $u$ of block \( \nabla f \) . Therefore this block can be treated as a blackbox,  represented in this package by oracles, data strucutres containing the relation and constraint information between expressions. Each oracle represent a class of function and can only exist if there exist interpolation conditions for said class.

Oracles can be sampled at an expression to return another expression, establishing the relation information between the two expressions.  In Figure~\ref{ex_analysis}, by calling the SectorBounded function, an oracle containing the interpolation conditions for 1 strong 10 smooth convex functions is created and labeled f'. The oracle is then sampled at points xs and x0 by defining f'(xs) and f'(x0) inside the labeling macro to create expressions $\nabla $f(x0) and $\nabla $f(xs)

\begin{figure}[!h]
	\begin{lstlisting}[mathescape]
@show(f'(x0))
(f')(x0) = $\nabla $f(x0)

Vector in R$^n$
  Label: $\nabla $f(x0)
  Oracles: $\nabla $f
  Associations: Dual => $\nabla $f(x0)$^*$

\end{lstlisting}
\caption{Example of an expression created by sampling an oracle}
\label{ex_sampling}
\end{figure}

As an oracle is sampled, Algorithm Analysis uses the oracle's set of interpolation conditions to create constraints on the expression the oracle is being sampled at and the result expression, which are x0 and xs in Figure~\ref{ex_analysis}.

\begin{figure}[!h]
	\begin{lstlisting}[mathescape]
constraints(x0^2)
Set of constraints with 3 elements:
	0 $<=$ 1.1 <$\nabla $f(x0),x0> + 2.0 <xs,x0> - 2.0 |x0|$^2$ + 1.1 <x0,$\nabla $f(x0)> - 2.0 |xs|$^2$ - 1.1 <xs,$\nabla $f(x0)> + 2.0 <x0,xs> - 0.2 |$\nabla $f(x0)|$^2$ - 1.1 <$\nabla $f(x0),xs>
	0 $<=$ R[|x0|$^2$ <xs,x0>; <x0,xs> |xs|$^2$]
	0 $<=$ R[|x0|$^2$ <$\nabla $f(x0),x0> <xs,x0>; <x0,$\nabla $f(x0)> |$\nabla $f(x0)|$^2$ <xs,$\nabla $f(x0)>; <x0,xs> <$\nabla $f(x0),xs> |xs|$^2$]
	\end{lstlisting}
	\caption{Example of constraints created by sampling an oracle}
	\label{ex_orc_constraints}
\end{figure}

Figure~\ref{ex_orc_constraints} is created by creating an oracle in the \texttt{SectorBounded} function class. The first constraint is the interpolation condition for the gradient to be sector bounded, given the interpolated points $x_0$ and $x_s$:
\begin{multline*}
	0 \leq 1.1\langle \nabla f(x_0),x_0\rangle + 2 \langle x_s,x_0\rangle - 2 \|x_0\|^2 + 1.1\langle x_0,\nabla f(x_0)\rangle - 2\|x_s\|^2 \\
	- 1.1\langle x_s,\nabla f(x_0)\rangle + 2 \langle x_0,x_s\rangle - 0.2 \|\nabla f(x_0)\|^2 - 1.1\langle \nabla f(x_0),x_s\rangle,
\end{multline*}
while the third constraint applied on the Gram matrix associated with the vectors $x_0$, $\nabla f(x_0)$, and $x_s$, constraining it to be positive semidefinite:
\[
	0 \preceq \bmat{\|x_0\|^2 & \langle \nabla f(x_0),x_0\rangle & \langle x_s,x_0\rangle \\ \langle x_0,\nabla f(x_0)\rangle & \|\nabla f(x_0)\|^2 & \langle x_s,\nabla f(x_0)\rangle \\ \langle x_0,x_s\rangle & \langle\nabla f(x_0),x_s\rangle & \|x_s\|^2}.
\]
The second constraint, while does not effect the mathematical derivation of a convergence rate guarantee for this class of function, is redundant and represent a part of the code that will need to be fixed before the Algorithm Analysis package can be completed.

\subsection*{Transpose oracle}

The transpose of a vector is coded in Julia to be an oracle, which is created when a vector expression is defined and is stored inside a wrapper data structure. During the inner product operation or norm operation, an oracle representing the transpose of a vector is sampled at another expression to create a scalar inner product expression. In this case, the oracle is not based on any class of function and create the constraint on the Gram matrix detailed in \texttt{\Cref{constraint}}.

\section{States} \label{states}
Algorithm Analysis represents the states of an algorithm using expressions. As the user inputs the algorithm being analyzed, an initial state is created and an updated state is defined as some algebraic combination of the initial state and the gradient. The relationship between a state and its next state can be defined using the \texttt{=>} operation inside the labeling macro, as can be seen in Figure~\ref{ex_analysis}, and the next state is stored in the \texttt{next} field of a state expression.

\section{Label macro}
Algorithm Analysis uses a macro to keep the process of providing inputs to the program simple as some of the rules of programming might be difficult for novice users to navigate. While the package still works without the label macro, it is made more accessible both in terms of entering input and interpreting results.

When an expression is defined inside the labeling macro, the expression object created is given the label based on the variable name used.	While x0 and y0 in Figure~\ref{ex_analysis} were labeled with its variable name, an expression defined outside of the labeling macro, as shown in Figure~\ref{ex_unlabeled} would not.
	\begin{figure}[!h]
		\begin{lstlisting}[mathescape]
x3 = R$^n$()
Vector in R$^n$
  Label: Variable{R$^n$}
  Associations: Dual => LinearFunctional{R$^n$}
\end{lstlisting}
\caption{Example of an unlabeled expression}
\label{ex_unlabeled}
\end{figure}
	In some special cases, expressions are automatically given labels that follow common math notation. For an expression created from sampling an oracle representing the gradient of a function $f$, Figure~\ref{ex_labeled} shows the macro would assign the expression the label $\nabla f(x0)$, a math symbol typically recognized as the gradient. Similarly, as shown in Figure~\ref{ex_transpose}, the transpose of a vector such as \texttt{x'} is automatically labeled as \texttt{x\textsuperscript{*}}), while Figure~\ref{ex_orc_constraints} shows that inner products such as \texttt{x'*y} gets labeled as \texttt{$\langle$x,y$\rangle$}.
	\begin{figure}[!h]
		\begin{lstlisting}[mathescape]
@show(f'(x0))
(f')(x0) = $\nabla $f(x0)

Vector in R$^n$
	Label: $\nabla $f(x0)
	Oracles: $\nabla $f
	Associations: Dual => $\nabla $f(x0)$^*$
		\end{lstlisting}
		\caption{Example of a labeled oracle}
		\label{ex_labeled}
	\end{figure}

\section{Constraints}
During the analysis process, constraints are created by interpolation conditions in section 3.2, as well as when the user defines constraints on the iterative algorithm being analyzed. In order to keep track of these constraints while keeping the process of defining them simple, the program uses data structures that include the scalar expression being constrained, and the constraint which define the expression to be in a cone. The list of constraints supported include:

\begin{description}
	\item[Equal to zero] Scalar expressions can be constrained to be equal to zero with the == 0 operation, in which case the expression is constrained to exist in the zero set cone.
	\item[Non-positivity or non-negativity] Scalar expressions can be constrained to be larger or equal to zero or less or equal to zero with the $\geq 0$ or $\leq 0$ operation, in which case the expression is constrained to exist in the positive orthant cone.
	\item[Positive or negative semidefinite] Symmetric matrices consisting of scalar expressions can be constrained to be positive semidefinite with the $\succeq 0$ or $\preceq 0$ operation, in which case the expression is constrained to exist in the positive semidefinite cone.
\end{description}

Constraints upon being defined are added to the \texttt{constraints} field of each variable expression that form the decomposition of the expression being constrained. Figure~\ref{ex_orc_constraints} is an example showing 3 constraints. In addition to being created by sampling oracles, constraints can also be defined by users. When analyzing any algorithm, constraints can be added to the initial condition of the algorithm. Any constraints added by the user is included in the formation of the optimization problem used to derive the performance bound.

\begin{figure}[h]
	\begin{lstlisting}[mathescape]
	@algorithm (x0-xs)^2 <= 1
	\end{lstlisting}
	\caption{Example of user added constraint}
	\label{ex_user_constraints}
\end{figure}

\section{Performance measure}
Part of the required inputs to perform analysis is the performance measure, the convergence rate of which the package finds the worst-case guarantee through algorithm analysis. In Figure~\ref{ex_analysis}, the performance measure is set as $ (x_0 - x_s) ^2 $, which is the norm or distance between the initial point and the goal. This means the convergence rate guarantee returned is that of the distance between the point updated using gradient descent after each iteration $ x_k $ and the goal $ x_s $. Depending on which criteria the user wishes to analyze the algorithm by, the performance measure can be modified so long as it is a scalar expression.

\section{JuMP modeling language}
Analysis of an algorithm's performance optimizing a function is itself an optimization problem as defined in Section~\ref{OpPro}, in which the function being minimized is the convergence rate while the constraints of the problem are the constraints created by the oracle and the user. Therefore, in order to formulate and solve optimization problems, the Algorithm Analysis package uses JuMP \cite{jump}, a modeling language specialized in mathematical optimization embedded in Julia as part of the analysis process. The tools and functionalities offered by JuMP enable and simplify the steps of creating and solving an optimization problem. These tools are:

\begin{description}
	\item[Modeling] An optimization problem created by JuMP would include variables and their constraints along with the problem to be optimized, all of which need to be passed on to the solver. JuMP works by creating a model in which every elements of a problem would be defined and categorized. Variables and their constraints can then be defined in the model to form the optimization problem.
	\item[Solver] JuMP supports a large list of open source and commercial solver, which are packages containing algorithms to find solutions to the optmization problem being formulated. While examples of analysis shown in this thesis uses the SCS \cite{SCS} solver, any JuMP supported solver capable of solving semidefinite problems can be used instead.
	\item[Solution] After an optimization problem has been formed and solved, the solver returns whether the constraints on the Lyapunov function holds, indicating whether or not the convergence rate chosen can be guaranteed. 	 
\end{description}

Suppose we have a trivial optimization problem: minimize $x + y$, subject to $x \geq 3$ and $y \geq 4$. It can easily be found the minimum value of $x + y$ is $7$. Figure~\ref{ex_jump1} shows how JuMP can optimize the problem.
\begin{figure}[ht]
	\begin{lstlisting}[mathescape]
	model = JuMP.Model(SCS.Optimizer)
	JuMP.set_silent(model)
	JuMP.@variable(model, x)
	JuMP.@variable(model, y)
	JuMP.@constraint(model, x $\geq$ 3)
	JuMP.@constraint(model, y $\geq$ 4)
	JuMP.@objective(model, Min, x + y)
	JuMP.optimize!(model)

	JuMP.objective_value(model)
	7.000038313789448
	\end{lstlisting}
	\caption{Example of JuMP minimizing an optimization problem}
	\label{ex_jump1}
\end{figure}
However, as discussed in Section~\ref{Lyapunov}, Algorithm Analysis find the performance guarantee by finding whether there exists an optimization varible with which the constraints of the problem are satisfied. To demonstrate this functionality, we can use the model used in Figure~\ref{ex_jump1} with the same constraints on $x$ and $y$, but instead of setting an objective to minimize $x + y$, set a constraint that $x + y = 6$. As there exist no $x$ and $y$ given the constraints applied on them which would satisfy the constraint $x + y = 6$, the optimization problem cannot be solved. Given this problem, JuMP optimizes the problem and return the termination code \texttt{INFEASABLE} to indicate that no solution can be found in Figure~\ref{ex_jump2}.
\begin{figure}[ht]
	\begin{lstlisting}[mathescape]
	JuMP.@constraint(model, x + y == 6)
	JuMP.optimize!(model)
	
	JuMP.termination_status(model)
	INFEASIBLE::TerminationStatusCode = 2
	\end{lstlisting}
	\caption{Example of JuMP certifying the feasibility an optimization problem}
	\label{ex_jump2}
\end{figure}