\chapter{Code Components}\label{chapter:code}

% To perform algorithm analysis with JuPE, the user needs to follow the folliowing 3 steps:
% \begin{enumerate}
% 	\item Choose from a supported list the class of function to be optimized.
% 	\item Define an algorithm to be analyzed .
% 	\item Specify a performance measure.
%   \end{enumerate}
 
% \begin{figure}[hbtp]
% 	\begin{lstlisting}[mathescape]
% m,L = 1,10
% $ \alpha $ = 2/(L+m)
% @algorithm begin

% 	f = DifferentiableFunctional{R$ ^n $}()
% 	xs = first_order_stationary_point(f)
% 	f' $ \in $ SectorBounded(m, L, xs, f'(xs))

% 	x0 = R$ ^n $()
% 	x1 = x0 - $ \alpha $*f'(x0)

% 	x0 => x1

% 	performance = (x1-xs)^2
% end

% @show rate(performance)
% \end{lstlisting}
% \caption{Analysis Example
% \label{ex_analysis}
% \end{figure}


% In the example code, the user:
% \begin{itemize}
% 	\item Define the class of function f \comment{(use $f$ for mathematical symbols, or \texttt{f} for programming symbols)} and its gradient f' by calling one of the provided functions. In this example, the class of function is 1 - 10 sector bounded functions. \comment{Need to define what this means.}
% 	\item Set the global minimum goal as a stationary point $ x_s $\comment{.}
% 	\item Define an initial state $ x_0 $ and the algorithm with which its next state $ x_1$ is updated. In this example, the algorithm being analyzed is gradient descent with a step size $ \alpha $= 2/11 \comment{(include all math, such as $=2/11$, in dollar signs)}.
% 	\item Set the performance measure as the norm distance between the initial state and the goal $ (x_0 - x_s)^2 $. The returned convergence rate guarantee is the rate at which the performance measure is decreasing after each iteration of the algorithm.
% 	\item Call the rate function \comment{to} perform automated algorithm analysis.
% \end{itemize}

% With the calling of the rate function, JuPE derives every necessary input from the performance measure and performs analysis to return a rate of 0.6687164306640625. This means for the provided algorithm and every function in the class, the convergence rate of the performance measure is guaranteed to match or exceed the result worst-case guarantee rate. \comment{Need to describe what exactly is meant by the convergence rate. In particular, the analysis returns the scalar $\rho$ such that, for some constant $c>0$, the performance measure is upper bounded by $c \rho^k$ at each iteration $k$. If you would like to use ``big-oh'' notation, this means that the performance measure converges with rate $O(\rho^k)$.}
 
% \begin{figure}[hbtp]
% \begin{lstlisting}
% rate(performance) = 0.6687164306640625
% \end{lstlisting}
% \caption{Analysis result}
% \label{ex_result}
% \end{figure}
The package derives a guaranteed worst-case convergence rate by following the set of instructions presented in \Cref{chapter:lyapunov}, creating and solving an optimization problem to derive a performance certification.

Implementing a mathematical procedure into a program presents a list of challenges, such as enabling the program to understand and differentiate between variables representing concepts such as gradients or states. Another challenge is the formulation and solving of a convex optimization problem. Meanwhile, the goal of creating a simple user experience has to be fulfilled. This chapter goes into the code that constitutes the package and enables these functionalities.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Expressions}

Expressions are data structures used to represent variables created in the analysis, such as the state of an algorithm, or the inner product of two state vectors. Expression enables the implementation of the elements presented in \Cref{chapter:lyapunov} into a computer program.

\subsection*{Expression types}
The \texttt{AlgorithmAnalysis,jl} package defines the field \texttt{R} as a concrete expression type representing real scalar expressions. For vector expressions, they can exist in either one of two vector spaces \texttt{R$^n$} and \texttt{R$^m$} defined in the package. The analysis of GD, HB, and FG creates scalar expressions in \texttt{R} and vector expressions in the same vector space \texttt{R$^n$}.

State vectors and the gradient of a function at a state vector are represented by vector expressions in the \texttt{R$^n$} vector space by the package, while the value of a function evaluated at a vector are scalar expressions in \texttt{R}. Inner products of two vectors in the same vector space or the square norm of a vector, which appear in \eqref{eqn:int_cond} are also scalar expressions in \texttt{R}.

A vector expression can be defined with the operator \texttt{R$^n$()} and a scalar expression with \texttt{R()}

\begin{figure}[h]
	\begin{lstlisting}[mathescape]
@algorithm begin
x0 = R$^n$()
y0 = R()
end
@show(x0)

Vector in R$^n$()
Label: x0
Associations: Dual => x0$^*$

@show(y0)

Scalar in R
Label: y0
	\end{lstlisting}
\caption{Example of a real and vector expressions}
\label{ex_field}
\end{figure}

\subsection*{Variable and decomposition expressions}
Expressions can either be a variable expression or a decomposition expression representing some combination of variable expressions. An expression's decomposition is stored in its \texttt{value} field.
\begin{description}
	\item[Variable expression] is defined to be in a field or vector space and represents either a vector or scalar. A variable expression's decomposition is itself. The expressions \texttt{x0} and \texttt{y0} created in \cref{ex_field} are examples of a variable expression.

	\item[Decomposition expression] is a combination of variables or other decomposition expressions in the same field or vector space. Its decomposition is a dictionary containing how many of each variable expression form the decomposition expression.

\begin{figure}[h]
		\begin{lstlisting}[mathescape]
@algorithm begin
	x0 = R$^n$()
	x1 = R$^n$()
@show x0 + 2*x1
end

Vector in R$^n$
  Decomposition: x0 + 2 x1
  Associations: Dual => LinearFunctional{R$^n$}
		\end{lstlisting}
	\caption{Example of a decomposition expression}
	\label{ex_decomposition}
\end{figure}
\end{description}

\subsection*{Gram matrix}
A Gram expression data structure is used to represent Gram matrices. It contains the vector of vector expressions whose inner products make up the element of the Gram matrix. \cref{ex_Gram} shows the expression representing matrix \texttt{[|x0|² \ \textlangle x0,x1\textrangle; \textlangle x0,x1\textrangle \ |x1|²]}.

\begin{figure}[h]
	\begin{lstlisting}[mathescape]
@show Gram([x0, x1])
Gram([x0, x1]) = Gram matrix of vector R$^n$[x0, x1]

Gram matrix in Gram
	Value: R$^n$[x0, x1] $\otimes$ R$^n$[x0, x1]
	\end{lstlisting}
\caption{Example of a Gram matrix expression}
\label{ex_Gram}
\end{figure}

\section{Algebra} \label{sec:algebra}
Expressions in the package belong in an inner product space, which is a set of elements that can be vectors or scalar and which supports certain operations such as norm and inner product in addition to algebraic operations.

\texttt{AlgorithmAnalysis.jl} supports operations that characterize inner product spaces between expressions. The examples that demonstrate these operations use vector expressions \texttt{x0} and \texttt{x1} in \cref{ex_decomposition}.

\subsection*{Addition or subtraction between expressions}

Vectors and numbers can be added together in an inner product space if they are both vector expressions or scalar expressions. In an addition operation, the result is a new expression whose decomposition is the merging of the decomposition dictionaries of the expressions being added. Subtraction works the same as addition, except the values in the decomposition dictionary of the term being subtracted have their signs flipped.

\begin{figure}[!h]
	\begin{lstlisting}[mathescape]
a = x0+x1-x0-x0
@show(a)

Vector in R$^n$
	Label: a
	Decomposition: x1 - x0
	Associations: Dual => a$^*$
	\end{lstlisting}
	\caption{Example of addition and subtraction operation}
	\label{ex_addsub}
\end{figure}

\subsection*{Multiplication or division between an expression and a scalar} 
Vectors and scalars can be scaled by a real number (not a scalar expression) in the package. The package performs the multiplication or division of an expression by scaling its value when scaling a variable expression and scaling the value of its decomposition dictionary if it is a decomposition expression.

\begin{figure}[!h]
	\begin{lstlisting}[mathescape]
c = x0*-3 + x1*2
@show(c)

Vector in R$^n$
	Label: c
	Decomposition: 2 x1 - 3 x0
	Associations: Dual => c$^*$

	\end{lstlisting}
	\caption{Example of multiplication operation}
	\label{ex_scalar}
\end{figure}

\subsection*{Transpose of a vector}
When a vector expression is created, its transpose is also created in the \texttt{associations} field, allowing the program to keep track of whether an expression is the transpose of another. In the AlgorithmAnalysis.jl package, the transpose of a transpose expression returns the original expression.

\begin{figure}[!h]
	\begin{lstlisting}[mathescape]
@show(x0')

Oracle
	Description: Linear functional on R$^n$
	Label: x0*
	Properties: Linear()

@show(x0'')

Vector in R$^n$
	Label: x0
	Associations: Dual => x0$^*$
	\end{lstlisting}
	\caption{Example of transpose operation}
	\label{ex_transpose}
\end{figure}

\subsection*{Inner product operation between two vectors}
In an inner product space, the inner product operation of two vectors is possible and results in a scalar. For example, the inner product of vectors x1 and x0, denoted $\innerproduct{x1}{x0}$, is calculated as $x0^\tp * x1$.

\begin{figure}[!h]
	\begin{lstlisting}[mathescape]
inner = x0'*x1
@show(inner)

Scalar in R
	Label: <x0,x1>
	Oracles: x1$^*$

	\end{lstlisting}
	\caption{Example of an inner product between two vector expressions}
	\label{ex_inner}
\end{figure}

The ordering of vector inner products can result in two variables with different labels and perceived by the program as two different objects even though they represent the same mathematical inner product. For example, expressions labeled \texttt{<x1, x0>} and \texttt{<x0, x1>} are both created by taking the inner product of \texttt{x0} and \texttt{x1} but is not understood by the package as the same expression. This was shown during the testing of the package to affect the robustness and accuracy of the analysis through testing during the development of the package. A function is used to enforce a standard ordering based on hash: it determines the order of the inner product operation by comparing each expression's hash, ensuring the inner products between any two vector expressions are consistently ordered.

\begin{figure}[!h]
	\begin{lstlisting}[mathescape]
@show (x0'*x1, x1'*x0) # Create two identical expressions

(<x0,x1>, <x0,x1>)

@show (x0'*x1 + x1'*x0)

Scalar in R
  Decomposition: 2 <x0,x1>
	\end{lstlisting}
	\caption{Example of consistent inner product labelling order}
	\label{ex_hashorder}
\end{figure}

\subsection*{Squared norm}
A vector expression can be squared to create its squared norm, a scalar expression.
\begin{figure}[!h]
	\begin{lstlisting}[mathescape]
norm = x0^2

@show(norm)
norm = |x0|$^2$
Scalar in R
	Label: |x0|$^2$
	Oracles: x0$^*$
\end{lstlisting}
\caption{Example of norm of vector expression}
\label{ex_norm}
\end{figure}

\subsection*{Outer product}
% In the AlgorithmAnalysis.jl package, the outer product operation is used to construct Gram matrices from vectors of expressions. This functionality plays a crucial role in formulating the interpolation conditions and Lyapunov inequalities described in Chapter 3.
The outer product of a vector of vector expressions in the same inner product space can be found using the $\otimes$ function. It is used to construct a Gram matrix from two vectors of vector expressions.

\begin{figure}[!h]
	\begin{lstlisting}[mathescape]
op = [x0, x1] $\otimes$ [x0, x1]

@show op
op = R[|x0|$^2$ <x0, x1>; <x0, x1> |x1|$^2$]

2x2 Matrix{R}:
 |x0|$^2$    <x0, x1>
 <x0, x1>  |x1|$^2$

\end{lstlisting}
\caption{Example of outer product}
\label{ex_outerproduct}
\end{figure}

\section{Oracles} \label{sec:oracles}

As mentioned in \cref{constraint}, algorithm analysis of first-order systems relies on interpolation conditions --- for every state at which the gradient of the system is taken, constraints are applied to the state vector, the function value at said vector, and the gradient at that vector. To implement these interpolation conditions, the package uses oracles, data structures containing the relation and constraint information between expressions. Each oracle represents a class of function and can only exist if there exist interpolation conditions for said class.

Oracles can be sampled at one expression to return another, establishing the relation information between the two expressions.

When the oracle \texttt{f} representing a differentiable function is created, its corresponding gradient oracle \texttt{f'} is automatically created and given the property of being the gradient of \texttt{f}.

The oracle can be sampled at different expressions. For example, \cref{ex_sampling} shows how the oracle \texttt{f'} is sampled by defining \texttt{f'(x0)} and \texttt{f'(xs)} inside the labeling macro to create expressions \texttt{$\nabla $f(x0)} and \texttt{$\nabla $f(xs)}. This sampling can also be seen in \cref{ex_analysis}.

\begin{figure}[!h]
	\begin{lstlisting}[mathescape]
@algorithm begin
	f = DifferentiableFunctional{R$^n$}()
	f $\in$ SmoothStronglyConvex(1, 10)
	x0 = R$^n$()
	xs = first_order_stationary_point(f)
	f'(x0)
	f'(xs)
end
\end{lstlisting}
\caption{Example of sampling an oracle}
\label{ex_sampling}
\end{figure}

An oracle stores information about every expression at which it has been sampled along with their corresponding output. \cref{ex_inputs_outputs} shows the the input and output information of the oracle \texttt{f'} defined in \cref{ex_sampling}.

\begin{figure}[!h]
	\begin{lstlisting}[mathescape]
inputs_outputs(f')

(R$^n$[xs, x0], R$^n$[0, $\nabla$f(x0)])
\end{lstlisting}
\caption{Example of the inputs outputs information of an oracle}
\label{ex_inputs_outputs}
\end{figure}

In \cref{ex_sampling}, by defining the DifferentiableFunctional oracle \texttt{f} as belonging to the 10 smooth 1 strongly convex class of functions, the convex property is assigned to \texttt{f}. This information is also stored in the oracle.

\begin{figure}[!h]
	\begin{lstlisting}[mathescape]
f

Oracle
	Description: Differentiable functional on R$^n$
	Label: f
	Properties: 10-smooth, 1-strongly convex
	Associations: Gradient => $\nabla$f

f'

Oracle
	Description: Map from R$^n$ to R$^n$
	Label: $\nabla$f
	Properties: Empty set of properties
	Associations: GradientOf => f
\end{lstlisting}
\caption{Example of the properties of an oracle}
\label{ex_properties}
\end{figure}

During the analysis process, the package can derive the necessary information to create interpolation conditions from the oracles: the property of the oracles and the interpolated points, or at which expression the oracle has been sampled. \cref{ex_orc_constraints} shows constraints created by sampling an oracle representing the 1 smooth 10 strongly convex function class at the interpolated points \texttt{x0} and \texttt{xs}. This matches the interpolation condition equation in \cref{eqn:int_cond}.

\begin{figure}[!h]
	\begin{lstlisting}[mathescape]
constraints(f')
Set of constraints with 2 elements:
  0 $\leq$ 0.9 f(x0) + 0.1 <x0,$\nabla$f(x0)> - 0.5 |xs|$^2$ - 0.1 <xs,$\nabla$f(x0)> - 0.9 f(xs) - 0.05 |$\nabla$f(x0)|$^2$ + <xs,x0> - 0.5 |x0|$^2$
  0 $\leq$ -0.9 f(x0) - 0.5 |xs|$^2$ + <x0,$\nabla$f(x0)> - <xs,$\nabla$f(x0)> + 0.9 f(xs) - 0.05 |$\nabla$f(x0)|$^2$ + <xs,x0> - 0.5 |x0|$^2$
	\end{lstlisting}
	\caption{Example of constraints created by sampling an oracle}
	\label{ex_orc_constraints}
\end{figure}

While this section focuses primarily on \texttt{DifferentiableFunctional} oracles and their associated gradients, which are most relevant to analyzing smooth strongly convex functions, the package also supports other types of oracles. These include general \texttt{Map}, \texttt{LinearMap}, and \texttt{SymmetricLinearMap} types, which are especially useful in the analysis of primal-dual algorithms and other composite optimization frameworks. Each oracle can also be associated with a wide variety of mathematical properties. Oracles representing function classes are not just limited to representing smooth strongly convex function classes, and can also support properties such as being sector-bounded or slope-restricted, as commonly encountered in robust control or nonlinear system analysis. The framework is extensible: any function property can be encoded into an oracle as long as its interpolation conditions are formally defined and provable.

% \begin{multline*}
% 	0 \leq 1.1\langle \nabla f(x_0),x_0\rangle + 2 \langle x_s,x_0\rangle - 2 \|x_0\|^2 + 1.1\langle x_0,\nabla f(x_0)\rangle - 2\|x_s\|^2 \\
% 	- 1.1\langle x_s,\nabla f(x_0)\rangle + 2 \langle x_0,x_s\rangle - 0.2 \|\nabla f(x_0)\|^2 - 1.1\langle \nabla f(x_0),x_s\rangle,
% \end{multline*}
% while the third constraint applied on the Gram matrix associated with the vectors $x_0$, $\nabla f(x_0)$, and $x_s$, constraining it to be positive semidefinite:
% \[
% 	0 \preceq \bmat{\|x_0\|^2 & \langle \nabla f(x_0),x_0\rangle & \langle x_s,x_0\rangle \\ \langle x_0,\nabla f(x_0)\rangle & \|\nabla f(x_0)\|^2 & \langle x_s,\nabla f(x_0)\rangle \\ \langle x_0,x_s\rangle & \langle\nabla f(x_0),x_s\rangle & \|x_s\|^2}.
% \]
% The second constraint, while does not effect the mathematical derivation of a convergence rate guarantee for this class of function, is redundant and represent a part of the code that will need to be fixed before the Algorithm Analysis package can be completed.

\subsection*{Transpose oracle}

The transpose of an expression is coded in Julia to be an oracle and is primarily used to support the inner product and square norm operations. In this case, an oracle representing the transpose of a vector is sampled at another expression to create a scalar inner product expression. The oracle representing the transposed expression has the \texttt{Linear} property and is used to create constraints on the Gram matrix detailed in \Cref{constraint}.

\begin{figure}[h!]
	\begin{lstlisting}[mathescape]
x0 = R$^n$()

@show x0'

Oracle
  Description: Linear functional on R$^n$
  Label: x0*
  Properties: Linear()
\end{lstlisting}
\caption{Example of a transpose oracle}
\label{ex_transposeoracle}
\end{figure}

\section{States} \label{states}
The \texttt{AlgorithmAnalysis.jl} package represents the states of an algorithm using vector expressions. The user inputs the algorithm by defining the initial states, and how the algorithm updates is defined by creating an updated state as an algebraic combination of the initial state and the gradient. The algorithm can only be defined if the relationship between a state and its next state is established. This is done in the \texttt{AlgorithmAnalysis.jl} package using the \texttt{=>} operation inside the \texttt{@algorithm} labeling macro, and the next state is stored in the \texttt{next} field of a state expression.

\begin{figure}[h!]
	\begin{lstlisting}[mathescape]
@algorithm begin
	x0 = R$^n$()
	x1 = x0 - $\alpha$*f'(x0)
	x0 => x1
end

@show x0

Vector in R$^n$
  Label: x0
  Next: x1
  Associations: Dual => x0*
	\end{lstlisting}
	\caption{Example of the \texttt{=>} operation}
	\label{ex_next!}
\end{figure}

When using the package, the user only has to define the next state relationship for the state expressions of the algorithm. For other expressions in the analysis, the package automatically assigns the relationship if it exists. For example, if a decomposition expression \texttt{y1} is defined as the sum of expressions \texttt{x0} and \texttt{x1}, and the next state of \texttt{x0} is \texttt{x1} and the next state of \texttt{x1} is \texttt{x2} while \texttt{y1}'s next state is not defined by the user, the package automatically define that next state it as an expression with decomposition \texttt{x1+x2}. This is shown in \cref{ex_nextdecomp}.

\begin{figure}[h!]
	\begin{lstlisting}[mathescape]
@algorithm begin
	x0 = R$^n$()
	x1 = R$^n$()
	x2 = R$^n$()
	y0 = x0 + x1
	x0 => x1
	x1 => x2
end

next(y0)

Vector in R$^n$
  Decomposition: x1 + x2
  Associations: Dual => LinearFunctional{R$^n$}
\end{lstlisting}
\caption{Example of automatic next state assignment for decomposition expressions}
\label{ex_nextdecomp}
\end{figure}

Automatic next state assignment is also performed on the output of sampled oracles. For example, if an oracle \texttt{f} is sampled at \texttt{x0} and its next state \texttt{x1}, the output expression \texttt{f(x1)} is automatically assigned as the next state of \texttt{f(x0)}.

\begin{figure}[h!]
	\begin{lstlisting}[mathescape]
@algorithm begin
	x0 = R$^n$()
	x1 = R$^n$()
	f'(x0)
	f'(x1)
	x0 => x1
end

next(f'(x0))

Label: $\nabla$f(x1)
  Oracles: $\nabla$f
  Associations: Dual => $\nabla$f(x1)
\end{lstlisting}
\caption{Example of automatic next state assignment for oracle output expressions}
\label{ex_nextsample}
\end{figure}

\subsection*{First order stationary point}

To represent the minimizer of a function, the user can use the function \texttt{first\_order\_\allowbreak stationary\_point()} to create the minimizer vector expression. This function is a convenient way to create a vector whose next state is itself, and where the gradient is zero.

\begin{figure}[h!]
	\begin{lstlisting}[mathescape]
@algorithm begin
f = DifferentiableFunctional{R$^n$}()
xs = first_order_stationary_point(f)

@show next(xs), f'(xs)
(xs, 0)

\end{lstlisting}
\caption{Example of a minimizer expression}
\label{ex_minimizer}
\end{figure}

\section{Automatic lifting}

When using the \texttt{AlgorithmAnalysis.jl} package, the user can call the \texttt{lift} function to automatically create additional state iterates which will be a part of the analysis. Once the algorithm being analyzed has been defined, the latest iterate and the desired number of additional states or \texttt{lifting\_dimension} can be provided as arguments to the \texttt{lift} function, which create the additional iterates in three steps:

\begin{enumerate}
	\item State and input extraction: Given the latest iterate, its decomposition is split between inputs --- expressions which are the output of sampling an oracle, and states --- any remaining expression in the decomposition. The states present in the decomposition of the latest iterates are sorted by their next value similar to a singly-linked list to create a list called \texttt{initial\_state}: The first element of the list is the head, which is the first iterate that no other state expression points to as the next state. Each successive state in \texttt{initial\_state} is the next state of the last element in the list, excluding the latest state.
	\item Formula extraction: A state formula is created, consisting of scalars to multiply elements in \texttt{initial\_state} and the inputs in order to create the latest iterate. Similarly, an input formula is created to express how each input is created using the \texttt{initial\_state}.
	\item Creating new state iterate: To create the next iterate, \texttt{initial\_state}'s first element is removed and the lastest state appended. The input formulas can be applied to this updated state list to create the latest inputs. The updated state list and latest inputs are then used to create the newest state iterate, following the state formula. 
\end{enumerate}

This process is placed in a loop to ensure the number of additional states created matches the \texttt{lifting\_dimension} value given by the user. Every new state created by the \texttt{lift} function is given the appropriate next state connection. Every new states and inputs are given labels \texttt{"(i)th\_lift\_state"} and \texttt{"(i)th\_lift\_(j)th\_input"} respectively, where \texttt{i} is the total number of existing state plus one and \texttt{j} minus one is the number of input that has already been created to form the latest state.

\section{Special syntax operations}

The \texttt{AlgorithmAnalysis.jl} package contains functions that are named using math notation symbols and overloads Julia base functions to create a simplified user experience. These functions include:
\begin{itemize}
\item \textbf{Algebraic}: As shown in \cref{sec:algebra}, Julia base functions \texttt{+}, \texttt{-}, \texttt{*}, \texttt{/}, and \texttt{\textasciicircum} are overloaded to support algebraic operations. Therefore, the package executes the function \texttt{x0+y0-x0-x0} in \cref{ex_addsub} using its addition and subtraction functions meant for expression variables instead of Julia's base functions.
\item \textbf{Transpose and Gradient}: The AlgorithmAnalysis.jl package follows math notation in that both the transpose of a function and the gradient of a function are denoted using the operation \texttt{'} in the package. And since in Julia, the operator \texttt{'} uses the \texttt{adjoint} function, we overload the function in the package with functions that allow the user to refer to the transpose of a vector expression when the operator \texttt{'} is placed after a vector expression. On the other hand, when used on one of the package's supported differentiable function oracles, the operator denotes the gradient of a function.
\item \textbf{Sampling functions and gradients}: By sampling an oracle at input an expression, we get an output expression that together with the input expression is constrained according to the interpolation condition. The package overloads the \texttt{()} operation to sample an oracle. For example, we sample a \texttt{DifferentiableFunctional} oracle \texttt{f} at expression \texttt{x0} simply by calling \texttt{f(x0)}.
\item \textbf{Sampling transpose oracles}: As the transpose of a vector expression is considered an oracle, inner product or norm expressions are created by sampling an oracle. Therefore, the package overloads the \texttt{*} operation when used between a transpose oracle and a vector expression to denote sampling that oracle, facilitating the inner product operation.
\item \textbf{Oracle property}: The function class is a necessary requirement to identify the property of an oracle and therefore its associated interpolation conditions. Therefore, the package uses the operation \texttt{$\in$} to denote an oracle belonging to one of the package's supported classes. In \cref{ex_in}, the function \texttt{f} is defined to be in the class of m smooth L strongly convex functions which informs the analysis process on which set of interpolation conditions to apply.
% \item \textbf{Sampling maps}:
\end{itemize}

\begin{figure}[h!]
	\begin{lstlisting}[mathescape]
f = DifferentiableFunctional{R$^n$}()
f $\in$ SmoothStronglyConvex(m, L)
	\end{lstlisting}
	\caption{Example of the $\in$ operation}
	\label{ex_in}
\end{figure}

\section{Labeling expressions}

The package uses a macro to keep the process of providing inputs to the program simple as some of the rules of programming might be difficult for novice users to navigate. While the package still works without the label functionalities, it is made more accessible both in terms of entering inputs and interpreting results produced by the package.

\subsection*{@algorithm label macro}

When using the AlgorithmAnalysis.jl package, the user defines the algorithm, function class, and the performance measure inside of the \texttt{@algorithm} macro. It is the main way through which the process of providing input for the analysis is simplified.

When an expression is defined inside the labeling macro \texttt{@algorithm}, the expression object created is given the label based on the variable name used.

\begin{figure}[h!]
	\begin{lstlisting}[mathescape]
@algorithm x0 = R$^n$()

@show x0

Vector in R$^n$
  Label: x0
  Associations: Dual => x0*
	\end{lstlisting}
	\caption{Example of a labeled expression}
	\label{ex_labeled}
\end{figure}

However, an expression defined outside of the labeling macro, as shown in \cref{ex_unlabeled}, would be given a default label.

\begin{figure}[h!]
	\begin{lstlisting}[mathescape]
x3 = R$^n$()

@show x3

Vector in R$^n$
Label: Variable{R$^n$}
Associations: Dual => LinearFunctional{R$^n$}
\end{lstlisting}
\caption{Example of an unlabeled expression}
\label{ex_unlabeled}
\end{figure}

The \texttt{@algorithm} macro is also responsible for simplifying the process of assigning an expression as the `next' of another. While in regular Julia syntax, the code \texttt{x0 => x1} simply creates a pair between the 2 variables, when executed inside the macro automatically assigns the expression \texttt{x1} as the next state of \texttt{x0}. This could be seen in \cref{ex_next!}.

\subsection*{Default labels}
In special cases, expressions are automatically given default labels that follow common math notation. This list includes:

\begin{itemize}
	\item \textbf{Transpose}: A transposed variable is labeled by appending an asterisk.
 For example, given a vector expression \texttt{x}, its transpose \texttt{x'} is labeled \texttt{x\textsuperscript{*}}.

	\item \textbf{Gradient}: A gradient is labeled using the nabla symbol. For example, given a DifferentiableFunctional Oracle \texttt{f}, its gradient \texttt{f'}  is labeled \texttt{$\nabla$ f}

    \item \textbf{Abstract Operator Application}: An abstract operator applied to an expression is labeled with parenthesis similar to the mathematical notation of a function. For example, if a DifferentiableFunctional Oracle \texttt{f} is sampled at vector expression \texttt{x0}, the resulting expression is labeled \texttt{f(x0)}

    % \item \textbf{Constant Map}: A Constant Map operator which maps any expression of a field to a unique element simply uses the Constant Map's label for any result we get from sampling it. For example, given a Constant Map \texttt{$\Sigma$} is sampled at vector expression \texttt{x0}, the resulting expression is labeled \texttt{$\Sigma$}.

    \item \textbf{Inner Product}: An inner product between two expressions is displayed using angled brackets. For example, the inner product of vector expressions \texttt{x0} and \texttt{x1} is labeled \texttt{<x0, x1>}
	\item \textbf{Squared norm}: An inner product between an expression and itself is displayed following squared norm math notation. For example, the square norm of vector expression \texttt{x0} is labeled \texttt{|x0|$^2$}
	\item \textbf{Oracle description}: Any created oracle is given a description based on its type, which is displayed when the oracle is accessed in the terminal.
	
\begin{figure}[!h]
	\begin{lstlisting}[mathescape]
f = DifferentiableFunctional{R$^n$}()
f $\in$ SmoothStronglyConvex(1, 10)
end

@show f

Oracle
	Description: Differentiable functional on R$^n$
	Label: f
	Properties: 10-smooth, 1-strongly convex
	Associations: Gradient => $\nabla$f
\end{lstlisting}
\caption{Example of an oracle's description}
\label{ex_description}
\end{figure}
\end{itemize}

\section{Constraints} \label{sec_constraints}
During the analysis process, constraints are created by interpolation conditions, as well as when the user defines constraints on the iterative algorithm being analyzed. In order to keep track of these constraints while keeping the process of defining them simple, the program uses data structures that include the scalar expression being constrained and the constraint itself. The package defines three constraints, which place the expression under it into one of three cones, which include:

\begin{description}
	\item[Equal to zero] Scalar expressions can be constrained to be equal to zero with the == 0 operation, in which case the expression is constrained to exist in the zero set cone.
	\item[Non-positivity or non-negativity] Scalar expressions can be constrained to be larger or equal to zero or less or equal to zero with the $\geq 0$ or $\leq 0$ operation, in which case the expression is constrained to exist in the positive orthant cone.
	\item[Positive or negative semidefinite] Symmetric matrices consisting of scalar expressions can be constrained to be positive semidefinite with the $\succeq 0$ or $\preceq 0$ operation, in which case the expression is constrained to exist in the positive semidefinite cone.
\end{description}

Constraints upon being defined are added to the \texttt{constraints} field of each variable expression that form the decomposition of the expression being constrained. \cref{ex_orc_constraints} is an example showing three constraints automatically created by an oracle's interpolation conditions. In addition to being created by sampling oracles, constraints can also be defined by users. When analyzing any algorithm, constraints can be added to the initial condition of the algorithm. Any constraints added by the user are included in the formation of the optimization problem used to derive the performance bound.

\begin{figure}[h]
	\begin{lstlisting}[mathescape]
@algorithm (x0-xs)^2 <= 1
0 $\leq$ 1 - |xs|$^2$ + 2 <x0,xs> - |x0|$^2$
	\end{lstlisting}
	\caption{Example of user added constraint}
	\label{ex_user_constraints}
\end{figure}

\section{Performance measure}
Part of the required inputs to perform the analysis is the performance measure. The worst-case convergence rate found by the analysis belongs to this performance measure. In \cref{ex_analysis}, the performance measure is set as $ (x_0 - x_s) ^2 $, which is the norm or distance between the initial point and the goal. This means the convergence rate guarantee returned is that of the distance between the point updated using gradient descent after each iteration $ x_k $ and the goal $ x_s $. Depending on which criteria the user wishes to analyze the algorithm by, the performance measure can be modified so long as it is a scalar expression.

\section{JuMP modeling language}
The search for a Lyapunov function that certifies a given convergence rate is an optimization problem, one where the parameters are the optimization variables $P$, $\lambda$, and $\mu$. Therefore, in order to formulate and solve optimization problems, the \texttt{AlgorithmAnalysis.jl} package uses JuMP \cite{jump}, a modeling language specialized in mathematical optimization embedded in Julia as part of the analysis process. The tools and functionalities offered by JuMP enable and simplify the steps of creating and solving an optimization problem. These tools are:

\begin{description}
	\item[Modeling] An optimization problem created by JuMP would include variables and their constraints along with the problem to be optimized, all of which need to be passed on to the solver. JuMP works by creating a model in which every element of a problem is defined and categorized. Variables and their constraints can then be defined in the model to form the optimization problem.
	\item[Solver] JuMP supports a large list of open-source and commercial solvers, which are packages containing algorithms to find solutions to the optimization problem being formulated. While examples of analysis  shown in this thesis and the results in \cref{chapter:results} use the Mosek \cite{mosek} solver, any JuMP-supported solver capable of solving semidefinite problems can be used instead.
	\item[Solution] After an optimization problem has been formed and solved, the solver returns whether the constraints on the Lyapunov function hold, indicating whether or not the convergence rate chosen can be guaranteed. 	 
\end{description}

The package has been tested with the SCS \cite{SCS} and Mosek solvers. Mosek was chosen over SCS as the SCS solver produced inaccurate performance guarantees in select scenario, which is discussed in \cref{chapter:conclusion}. It should be noted however that while the SCS can be installed for free in JuMP and Julia, Mosek requires a liscense, although a free academic license is available.
 
Suppose we have a trivial optimization problem: minimize $x + y$, subject to $x \geq 3$ and $y \geq 4$. It can easily be found the minimum value of $x + y$ is $7$. \cref{ex_jump1} shows how JuMP can optimize the problem.

\begin{figure}[ht]
	\begin{lstlisting}[mathescape]
model = JuMP.Model(SCS.Optimizer)
JuMP.set_silent(model)
JuMP.@variable(model, x)
JuMP.@variable(model, y)
JuMP.@constraint(model, x $\geq$ 3)
JuMP.@constraint(model, y $\geq$ 4)
JuMP.@objective(model, Min, x + y)
JuMP.optimize!(model)

JuMP.objective_value(model)
7.000038313789448
	\end{lstlisting}
	\caption{Example of JuMP minimizing an optimization problem}
	\label{ex_jump1}
\end{figure}

Lyapunov-based algorithm analysis finds the performance guarantee by certifying whether there exist optimization variables with which the optimization problem together with its constraints are satisfied. To demonstrate this functionality, we can use the model used in \cref{ex_jump1} with the same constraints on $x$ and $y$, but instead of setting an objective to minimize $x + y$, set a constraint that $x + y = 6$. As there exists no $x$ and $y$ given the constraints applied on them which would satisfy the constraint $x + y = 6$, the optimization problem cannot be solved. Given this problem, JuMP optimizes the problem and returns the termination code \texttt{INFEASABLE} to indicate that no solution can be found in \cref{ex_jump2}.

\begin{figure}[ht]
	\begin{lstlisting}[mathescape]
	JuMP.@constraint(model, x + y == 6)
	JuMP.optimize!(model)
	
	JuMP.termination_status(model)
	INFEASIBLE::TerminationStatusCode = 2
	\end{lstlisting}
	\caption{Example of JuMP certifying the feasibility an optimization problem}
	\label{ex_jump2}
\end{figure}