 \chapter{Literature Review}

In recent years, numerous studies have been conducted comparing the performance of optimization algorithms, particularly in machine learning and deep learning contexts. These comparisons often focus on convergence speed, accuracy, and robustness across various models and datasets. In \cite{adam}, Kingma and Ba presented the Adam (Adaptive Moment Estimation) algorithm. In order to demonstrate the Adam algorithm, the authors designed and conducted experiments where different optimization algorithms are used to solve popular machine learning models, the convergence rate of each recorded. The result of these experiments is emperical evidence providing a general idea on an algorithm's performance at solving different problems.

There exist in the literature many approaches to approaches to performing algorithm analysis. In 2014, Drori and Teboulle first introduced the method of representing a class of function with constraints, reformulating the problem of analyzing an optimization method into a semidefinite program (SDP) whose size is proportionate with the number of iterations the algorithm is run.\cite{drori2012}. The paper coined the term Performance Estimation Problem (PEP) and showed that by solving convex semidefinite problem, a worst-case numerical bound on an algorithm's performance solving that class of function can be derived. Taylor, Hendrickx and Glineur built upon this work by introducing the ideas of creating a finite representation for a class of smooth strongly convex functions using closed-form necessary and sufficient conditions. While the above mentioned two approaches give the performance bound in the form of a guarantee how close \(x_k\) is to the goal \(x_s\) after a fixed number of iterates, the method used in this package proves that the performance measure inputted by the users decreases at a guaranteed rate throught out the optimizing process.

In \cite{iqc}, Megretski and Rantzer demonstrated how integral quadratic constraints (IQCs) can be used to unify and simplify the analysis of system stability and performance. The paper shows how a complex system can be described using certain IQCs and presents a stability theorem for systems described by IQCs.

Computer programs have been developed to perform algorithm analysis using the PEP methods, which are PESTO \cite{pesto}, a MATLAB toolbox, and PEPit \cite{pepit}, a Python package. The program is able to perform algorithm analysis and generate a worst-case performance guarantee for algorithms and function classes from a supported list. PESTO and PEPit follows the PEP methodology and first presented an automatic way to analyze gradient-based algorithms.

The main contribution of this thesis paper is to create an computer program similar PESTO and PEPit that aims to provide an accessible and fast way to analyze the performance of first-order methods for a guaranteed convergence rate, leveraging the approach presented in \cite{tutorial}, in the Julia programming language.