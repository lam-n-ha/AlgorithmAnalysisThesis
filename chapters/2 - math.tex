\chapter{Literature Review}\label{chapter:literature}

In recent years, numerous studies have been conducted comparing the performance of optimization algorithms, particularly in machine learning and deep learning contexts. These comparisons often focus on convergence speed, accuracy, and robustness across various models and datasets. In \cite{adam}, Kingma and Ba presented the Adam (Adaptive Moment Estimation) algorithm. In order to prove its efficiency improvement above existing algorithms, the authors designed and conducted experiments where they are used to solve popular machine learning models and recorded the convergence rate of each. The result of these experiments provided emperical evidence that Adam performed better compared to its peers and gave a general idea of Adam's performance.

While the emperically proven performance of Adam has made it one of the most popular algorithms in machine learning, in \cite{adam2}, it is proved that under specific conditions, Adam is unable to converge on simple quadratic problems. This is something observational analysis did not demonstrate, highlighting its shortcomings and need for better analysis tools.

There exist in the literature approaches to analyzing algorithms which guarantee an algorithm's minimum performance. In 2014, Drori and Teboulle first introduced the method of representing a class of function with constraints, reformulating the problem of analyzing an optimization method into a semidefinite program (SDP) \cite{drori2012}. The paper coined the term Performance Estimation Problem (PEP) and showed that by solving a convex semidefinite problem, a worst-case numerical bound on an algorithm's performance solving that class of function can be derived. Taylor, Hendrickx and Glineur built upon this work by introducing the ideas of creating a finite representation for a class of smooth strongly convex functions using closed-form necessary and sufficient conditions \cite{pepit}. This work culminated in a way to perform algorithm analysis to derive the performance bound in the form of a guarantee that after a fixed number of iterates, how close the last iterate is to the goal.

In \cite{iqc}, Megretski and Rantzer demonstrated how integral quadratic constraints (IQCs) can be used to unify and simplify the analysis of system stability and performance. The paper introduced the idea that optimization algorithms can be interpreted as a dynamical systems in which the gradient of the objective function --- a complex system --- can be constrained to be in some class using IQCs, making IQCs one tool from \textit{robust control} that can be applied to study robust stability. Through this interpretation, the problem of analyzing algorithms' performance is transformed into a robust control problem, and analyzing the performance of the optimization algorithm is equivalent to analyzing the stability of the corresponding dynamical system. The first paper to apply IQCs from robust control to analyze the performance of optimization algorithms is~\cite{lessard2016}, a highly influential paper in this area. While both~\cite{lessard2016} and Drori and Teboulle's PEP method derive an SDP the solution of which guarantees precise bounds on the convergence rate of first order algorithms, a major drawback of the PEP method was that the SDP scales in size with the number of iterations the algorithm is run, making the analysis of algorithms which. In~\cite{lessard2016}, authors Lessard, Recht, and Packard demonstrate by using Lyapunov functions, a bound on an algorithm's convergence rate can be found by deriving a small and fixed size SDP.

The Algorithm Analysis package implements the algorithm analysis approach presented by Van Scoy and Lessard in \cite{tutorial}. This Lyapunov-based approach to analysis transforms the analysis problem into a robust control problem, similar to the IQC method, and uses interpolation conditions to describe the complex system that is the set of smooth strongly convex functions, similar to the PEP method. The method then forms convex SDP using Lyapunov functions, the solution of which establishes whether a convergence rate can be guaranteed for the given algorithm and problem class, making the derivation of the fastest guaranteed rate a simple search problem.

Computer programs have been developed to perform algorithm analysis using the PEP methods, which are PEPit \cite{pepit}, a Python package, and PESTO \cite{pesto}, a corresponding MATLAB toolbox. PESTO and PEPit follows the PEP methodology and first presented an automatic way to analyze gradient-based algorithms: Given an algorithm and problem class from a supported list, the programs can provide a guaranteed minimum convergence rate of the algorithm for every problems in the class.

The main contribution of this thesis paper is to create a computer program named Algorithm Analysis similar PESTO and PEPit that aims to provide an accessible and fast way to analyze the performance of first-order methods for a guaranteed convergence rate. By developing a domain-specific language inside the Julia programming language, Algorithm Analysis simplifies the process of defining an optimization algorithm, provides a systemic way to represent abstract concepts such as algorithm iterate or the gradient of an abstract function, and make the analysis of optimization algorithms more accessible to non-expert users.