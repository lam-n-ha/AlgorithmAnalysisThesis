\chapter{Analysis Process}\label{chapter:process}

As described in \Cref{chapter:lyapunov} The Lyapunov function approach certifies whether a given convergence rate can hold given the input by finding a Lyapunov function that both bounds the performance measure and converging at the given rate. The analysis process takes place in four steps:

\begin{enumerate}
    \item The components necessary to certify a convergence rate --- the algorithm, the constraints, and the performance measure --- are collected automatically to form a systematic characterization of the analysis problem, using the code structures described in \Cref{chapter:code}. This characterization includes using the state information to determine how the algorithm being analyzed updates, creating constraints on interpolated points based on the interpolation conditions of the oracles' properties.
    \item These data structures are converted to real number vectors and matrices in the form of a linear function of the initial states and inputs. These variables represent the algorithm's initial and updated states as well as the constraints created.
    \item An optimization problem is created using the representations created in the last step inside a JuMP model. The problem of finding a Lyapunov function is transformed into finding the optimal with which the problem is feasible, which proves whether a certain convergence rate is feasible for a given problem.
    \item The above three steps are repeated with different convergence rates as the program searches for the smallest feasible convergence rate using bisection search.
\end{enumerate}

This chapter details the analysis process of analyzing gradient descent's performance at optimizing any $10$ smooth $1$ strongly convex function as shown in \cref{ex_analysis}, including how the steps presented in \cref{chapter:lyapunov} are performed and how the optimization problem is formed and solved to derive worst-case performance convergence rate.

\section{Analysis problem formulation} \label{sec_collect}

Once the user has given the package the necessary input to perform analysis, they can call the \texttt{rate} function on the performance measure to begin analysis. The package begins the analysis automatically finding every variable that is part of the analysis problem using a recursive function.

This process starts with the system creating a set of expressions present in the performance measure, a set of their associated oracles, and a list of constraints associated with found expressions and oracles. The original set of expressions is then appended with expressions present in the constraints or sampled with the oracles, and the process is repeated until no new variable is found. Figure \ref*{ex_collect1} shows how every expression, constraint, and oracle is collected. 

\begin{figure}[h!]
    \begin{lstlisting}[mathescape]
vars, cons, orcs = variables_constraints_oracles(performance)

@show vars
Set{Expression} with 11 elements:
  <$\nabla$f(x0),xs>
  f(x0)
  |$\nabla$f(x0)|$^2$
  |xs|$^2$
  <x0,xs>
  |x0|$^2$
  f(xs)
  $\nabla$f(x0)
  <$\nabla$f(x0),x0>
  xs
  x0
\end{lstlisting}
\caption{Collected expressions, oracles, and constraints (part one)}
\label{ex_collect1}
\end{figure}

\begin{figure}[H]
\ContinuedFloat
\begin{lstlisting}[mathescape]
@show cons
Set of constraints with 3 elements:
  0 $\preceq$ Gram matrix of vector R$^n$[$\nabla$f(x0), xs, x0]
  0 $\leq$ -0.1 <$\nabla$f(x0),xs> - 0.9 f(xs) + 0.9 f(x0) - 0.05 |$\nabla$f(x0)|$^2$ + <x0,xs> + 0.1 <$\nabla$f(x0),x0> - 0.5 |xs|$^2$ - 0.5 |x0|$^2$
  0 $\leq$ -<$\nabla$f(x0),xs> + 0.9 f(xs) - 0.9 f(x0) - 0.05 |$\nabla$f(x0)|$^2$ - 0.5 |xs|$^2$ + <x0,xs> + <$\nabla$f(x0),x0> - 0.5 |x0|$^2$

@show orcs
Set{Oracle} with 5 elements:
  x0*
  $\nabla$f(x0)*
  $\nabla$f
  f
  xs*
\end{lstlisting}    
\caption{Collected expressions, oracles, and constraints (part two)}
\label{ex_collect2}
\end{figure}

\subsection*{Pruning Gram matrix constraints}
Due to the fact the package uses a recursive function to create the Gram matrix constraints that interpolate the inner product expressions present in the analysis problem, it in some cases creates more Gram matrix constraints than intended. These constraints are applied to Gram matrices that are the principal submatrix of the Gram matrix we wish to constrain. Since any principal submatrix of a matrix is positive semidefinite as long as the matrix is positive semidefinite, any constraint applied on the principal submatrix is redundant.% This means the vector of expressions they are the outer product of is a subset of the vector of expressions used to create the desired Gram matrix

While redundant constraints do not affect the accuracy of the certification of a convergence rate, they can significantly increase computational cost. As will be discussed in the next section, each constraint applied to a Gram matrix introduces a number of additional optimization variables equal to the size of the matrix. Therefore, including any unnecessary Gram matrix constraints leads to an increased problem size and can slow down the analysis process. To prevent this therefore the package prunes the list of constraints to remove any redundant constraints on the Gram matrix.

\section{Linear form transformation} \label{sec_linearform}

% In JuPE, when variable expressions are defined in an inner product space, they are vector. These expressions include states of an algorithm, the starting and end points, and the gradient of a function at a point. This is because a function class is multidimensional, which means each point or iterate is a vector whose size is identical to the dimension of the function. 

% All three components needed to form the linear matrix inequality - the Lyapunov functions $V(x_k)$ and $V(x_{k+1})$ and the left hand side of each constraint - from equations \ref{eqn:int_cond2}, \ref{eqn:Ly_ineq} and \ref{eqn:Ly_ineq2} are functions linear in the elements of the Gram matrix and optimization variables. On the other hand, the JuMP modeling language does not support the expression and constraint data structures presented in chapter 4, and the LMIs of \ref{Ly_ineq2} must be transformed into a function linear in the optimization variables and real numbers before it can be formed inside the JuMP model. 
As specified in \cref{Lyapunov}, the linear matrix inequalities are constructed from the linear form of the Lyapunov functions and the constraints as a function of the vector $[x; u]$ that represents initial states and inputs. This process is done in three steps, which are:
\begin{enumerate}
    \item Of every expression that has been created during the input process, define the initial state vector \texttt{x} as every real expression which contains another expression in its next field.
\begin{figure}[h!]
    \begin{lstlisting}[mathescape]
x  = collect(v for v $\in $ vars if !ismissing(next(v)) && v isa R)
4-element Vector{R}:
    f(xs)   
    |xs|$^2$ 
    |x0|$^2$
    <x0,xs>
\end{lstlisting}
\caption{Initial state real scalar expressions}
\label{ex_initstate}
\end{figure}

    \item The input vector u is then defined as every real expression that does not have a next state.
    \begin{figure}[h!]
        \begin{lstlisting}[mathescape]    
u  = collect(v for v $\in$ vars if ismissing(next(v)) && v isa R)
4-element Vector{Expression}:
    <$\nabla $f(x0),xs>
    <$\nabla $f(x0),x0>
    |$\nabla $f(x0)|$^2$
    f(x0)
        \end{lstlisting}    
    \caption{Input real scalar expressions}
    \label{ex_updatedstate_input}
    \end{figure}
    \item The initial state and input vector \texttt{[x; u]} is the code equivalent of $\bmat{x & u}^\tp$ and can be multiplied with a matrix of real numbers to create every expression required to form the linear matrix inequality. The transformation of a decomposition expression of Gram matrix expression into such real number matrix will be referred to as the linear form of an expression and is crucial to creating the semidefinite problem in JuMP.
\end{enumerate}

\begin{figure}[h!]
    \begin{lstlisting}[mathescape] 
linear_form = (linearform([x; u] => x0^2 - 3*(x0'*xs)))

@show linear_form

1x8 Matrix{Int64}:
 0  -3  1  0  0  0  0  0

@show linear_form*[x; u]

Scalar in R
  Decomposition: -3 <x0,xs> + |x0|$^2$
\end{lstlisting}    
\caption{Example of linear form of a scalar expression}
\label{ex_linearform}
\end{figure}

The package now transforms the necessary input into its linear form in preparation for the final step of formulating an optimization problem in JuMP.

\section{Formulating optimization problem}

\subsection*{Performance measure}
The linear form matrix of the performance measure is the first of the three components needed to form the Lyapunov function in \eqref{eqn:Ly_ineq}, we used $\|x_k - x_s\|^2$. For example, the performance measure in \cref{ex_analysis}, which is defined as \texttt{(x0-xs)$^2$} and which evaluates into \texttt{|x0|$^2$ - <xs, x0> - <x0, xs> + |xs|$^2$}, has the linear form presented in \cref{ex_linearform2}.

\begin{figure}[h!]
\begin{lstlisting}[mathescape]
$\mathcal{P} $ = vec(linearform( [x; u] => performance ))
print($\mathcal{P}$)
[-1, -2, 1, 0, 0, 0, 0, 0, 0]
\end{lstlisting}
\caption{Linear form matrix of expression \texttt{(x0-xs)$^2$}}
\label{ex_linearform2}
\end{figure}

\subsection*{Lyapunov function formulation}

% The user when defining the algorithm to be analyzed start by first defining an initial state, and it is updated using the gradient of the function taken at some point. In Figure \ref*{ex_analysis} the initial and updated states are \texttt{x0} and \texttt{x1} respectively.

The Lyapunov functions can be formed according to equations \eqref{eqn:Vx}. This is done by the package by first separating every real scalar expression, which are inner product, norms, and function value expressions, and excluding state vector expressions, into an initial state and an updated state by making the distinction between real expressions that have a next state and those that do not.

In order to perform linear form transformations, we have already defined \texttt{x} as the initial state in \cref{sec_linearform} as every real scalar expression with a next expression. We can then define the updated state \texttt{x$^+$} consisting of the next expression of every expression in the initial state. 

\begin{figure}[h!]
    \begin{lstlisting}[mathescape]
x$^+$ = next(x)

4-element Vector{R}:
    f(xs)       
    |xs|$^2$    
    |x0|$^2$ - 0.36363636363636365 <$\nabla $f(x0),x0> + 0.03305785123966942 |$\nabla $f(x0)|$^2$
    -0.18181818181818182 <$\nabla $f(x0),xs> + <xs,x0>
    
\end{lstlisting}
\caption{Updated state \texttt{x$^+$} real scalar expression}
\label{ex_nextstate}
\end{figure}

% It should be noted that in \cref{ex_analysis}, the gradient descent algorithm is defined with 1 initial state \texttt{x0} and 1 updated state \texttt{x1}, therefore every scalar expressions not associated with \texttt{x1} is in \texttt{x} and every scalar expressions associated with \texttt{x1} is in \texttt{x2}. However, if an algorithm updates using multiple state vectors or the gradient at an interpolated point consisting of multiple state vectors, such is the case in the Fast-gradient and Heavy-ball algorithms, these vectors will also have to be defined. The resulting additional expressions will still be collected in the step detailed in \cref{sec_collect} and successfully separated due to the automatic next state assignment detailed in section \cref{states}.

% \subsection*{State space matrix and their linear form formulation}
% When using the package, the algorithm to be analyzed is provided as input first by defining an initial state and how the next state is updated using the initial state and the gradient. The initial state is defined to be a vector in an inner product space, and the updated state is a linear function of one or multiple initial state and the gradient of the function evaluated at some point. While the gradient descent algorithm updates using only one state and evaluate the gradient at the previous state, if an algorithm updates using multiple past states or the gradient at an interpolated point, these vectors will also have to be defined. The forming of the algorithm can then be completed by defining the relationship between states and their next states using the \texttt{=>} operation, which updates the next field of every expression in the decomposition of which there is the state on the left hand side of the operation. For example, in Figure~\ref{ex_analysis}, the state vector \texttt{x1} is defined as a function of the state vector \texttt{x0} and as the next state of \texttt{x0}, while the next state of the stationary point \texttt{xs} is itself. This not only means the \texttt{next} field of \texttt{x0} and \texttt{xs} are \texttt{x1} and \texttt{xs} respectively, but also next state of any every expression derived from the norm, inner product, or algebraic calculation of which \texttt{x0} is a part is that calculation done with \texttt{x1} instead. In this example, as shown in Figure~\ref{ex_next}, the next state of the inner product of $x_0$ and $x_s$ denoted as \texttt{next(x0'*xs)} is the inner product of $x_1$ and $x_s$ \texttt{x1'*xs}. This enables the operation in Figure~\ref{ex_linearform_next_state} and allow any updated iterate to be automatically expressed a linear function of the intial states and inputs.
% \begin{figure}[h!]
% 	\begin{lstlisting}[mathescape]
% next(x0)

% Vector in R$^n$
%   Label: x1
%   Decomposition: -0.18181818181818182 $\nabla $f(x0) + x0
%   Associations: Dual => x1*

% next(x0'*xs)

%   Scalar in R
%     Decomposition: -0.18181818181818182 <xs,$ \nabla $f(x0)> + <xs,x0>
% \end{lstlisting}
% \caption{\texttt{next} field of state a vector expression and a scalar formed from a state expression}
% \label{ex_next}
% \end{figure}

The initial state expressions \texttt{x} defined in \cref{ex_initstate} and the updated state real expressions \texttt{x$^+$} defined in \cref{ex_nextstate}, are then transformed into their linear form matrices. This is the second of the three components needed to formulate the Lyapunov function and is shown in \cref{ex_linearform_initstate} and \cref{ex_linearform_nextstate}.

\begin{figure}[h!]
    \begin{lstlisting}[mathescape]
X  = linearform([x; u] => x)
    4x9 Matrix{Int64}:
    1  0  0  0  0  0  0  0  0
    0  1  0  0  0  0  0  0  0
    0  0  1  0  0  0  0  0  0
    0  0  0  1  0  0  0  0  0    
    \end{lstlisting}
    \caption{Linear form state matrix x}
    \label{ex_linearform_initstate}
\end{figure}
\begin{figure}[h!]
    \begin{lstlisting}[mathescape]
X$^+$ = linearform([x; u] => x$^+$)
4x8 Matrix{Real}:
1  0  0  0   0          0          0          0          0
0  1  0  0   0          0          0          0          0
0  0  1  0   -0.363636  0.0330579  0          0          0
0  0  0  1   0          0          0          0         -0.1818
\end{lstlisting}
\caption{Linear form state matrix x$^+$}
\label{ex_linearform_nextstate}
\end{figure}
% \subsection*{State space in Lyapunov function}
Following the steps presented in \eqref{eqn:Ly_ineq} and \eqref{eqn:Vx} of \cref{chapter:lyapunov}, the Lyapunov function can be formed by first defining an optimization variable $P$ in the JuMP model as a JuMP variable. Once JuMP and the solver start optimizing the problem, P is one of the variables that will be optimized to certify a convergence rate. The code that creates the JuMP model and the formulation of the Lyapunov functions is presented in Figure \ref*{ex_lycode}.

\begin{figure}[h!]
    \begin{lstlisting}[mathescape]
# Create the JuMP model
model = JuMP.Model(SCS.Optimizer) 
JuMP.set_silent(model)
# Create Optimization variables
JuMP.@variable(model, P[1:length(x)]) 
# Create Lyapunov functions
V = X'*P
V$^+$ = X$^+$'*P
L1 = $\mathcal{P}$ - V
L2 = V$^+$ - $\rho$$^2$*V
\end{lstlisting}
\caption{Formation of optimization problem in JuMP}
\label{ex_lycode}
\end{figure}

\section{Constraints}

The last of the three components needed to perform analysis, the constraints are also transformed into their linear form and added to the optimization in three steps:
\begin{description}
\item [Optimization variable multipliers] For each constraint $i$, two optimization variables $\lambda$ and $\mu$, which represent $\Lambda^1_i$ and $\Lambda^2_i$ in \Cref{chapter:lyapunov} are defined as JuMP variables.
 If the constraint is applied to the Gram matrix, the optimization variables will be two matrices of the same size. Otherwise, if the constraint is created from the interpolation conditions of the class of function and is applied to a single real scalar expression, the optimization variables will have a size of $1$.
\item [Constraint on multiplier] The JuMP variables multipliers are constrained in the JuMP model depending on the constraint expression they were created for: The multiplier is not constrained if the expression is constrained to be zero, constrained to be non-negative if the expression is constrained to be non-negative, and constrained to be symmetrical and in the JuMP supported positive semidefinite cone if the expression is constrained to be positive semidefinite.
\item [Transformation into the linear form of constraints] For each constraint, the expression being constrained is scaled by the two optimization variable multipliers, before being transformed into their linear form. The two resulting matrices of multipliers are added to \texttt{L1} and \texttt{L2} respectively.
\end{description}

% If the expression constrained is a single real scalar, the linear form of the constraint is derived similarly to the linear form of the performance measure or state space matrices but scaled by the multiplier. Suppose we have the constraint $(x0 - xs)^2 \geq 0$ and matrix $\bmat{x\\ u} = \bmat{|x0|^2\\ <xs, x0>\\ |xs|^2}$, the linear form of the constraint in terms of $\bmat{x\\ u}$, denoted as $M$ would be:

% \begin{subequations} \label{eqn:constraint_single}
% 	\begin{align}
%     \lambda * (x0 - xs)^2 &= M * \bmat{|x0|^2\\ <xs, x0>\\ |xs|^2} \label{eq_cons_single1}       \\
% 	M &= \bmat{\lambda& 2\lambda & \lambda} \label{eq_cons_single2}
% 	\end{align}
% \end{subequations}

% If the expression constrained and its corresponding multiplier are vectors of expression, the linear form of the constraint is derived as the linear form of the inner product between the multiplier vector and the constraint expression vector. Suppose we have a constraint vector $\bmat{(x0-xs)^2 \\ (x0-xs)^2-3*|xs|^2} \geq 0$ and the same $\bmat{x\\ u}$ matrix as \eqref{eqn:constraint_single}, the linear form of the constraint in terms of $\bmat{x\\ u}$, denoted as $M$ would be:

% \begin{subequations} \label{eqn:constraint_vector}
% 	\begin{align}
%     \bmat{\lambda  & \lambda } * \bmat{(x0 - xs)^2 \\ (x0 - xs)^2-3*|xs|^2} &= M * \bmat{|x0|^2\\ <xs, x0>\\ |xs|^2} \label{eq_cons_vector1}       \\
% 	M &= \bmat{\lambda & -\lambda & \lambda \\ \lambda & -\lambda & -2\lambda} \label{eq_cons_vector2}
% 	\end{align}
% \end{subequations}

% And if the expression constrained and its corresponding multiplier are matrices, the linear form of the constraint is the linear form of the trace of the matrix multiplication between the multiplier and the constraint expression. For the Gram matrix in \eqref{eqn:trans_cond} which is constrained to be positive semidefinite, its linear form would be:

% \begin{equation} \label{eqn:trans_cond}
% 	tr(\lambda \bmat{||x0||^2 & \innerproduct{xs}{x0} & \innerproduct{\nabla f(x0)}{x0} \\ \innerproduct{x0}{xs} & ||xs||^2 & \innerproduct{\nabla f(x0)}{xs} \\ \innerproduct{x0}{\nabla f(x0)} & \innerproduct{xs}{\nabla f(x0)} & ||\nabla f(x0)||^2}) \geq 0	
% \end{equation}

% Where $\lambda $ is a 3x3 JuMP variable. In all three cases, for each constraints, 2 identical linear form matrices are created, one scaled by \texttt{$\lambda$} and added to the first Lyapunov function and the other by \texttt{$\mu$} and added to the second Lyapunov function. This completes the final linear matrix inequalities as defined in \eqref{eqn:LMI1}.

\section{Derived feasibility and bisection search}
Upon the completion of the linear matrix inequalities, the solver of the JuMP model is called to optimize the problem and find the variables \texttt{$P$}, \texttt{$\lambda$}-s and \texttt{$\mu$}-s for which the linear matrix inequality is satisfied and a convergence rate $\rho$ can be guaranteed.

In order to find the worst-case performance rate, the program performs a bisection search, also known as binary search, for the smallest value $\rho$ between 0 and 1 that makes the optimization problem feasible, calling a function to perform the steps presented in this chapter for each value \texttt{$\rho$} and checking feasibility at each iterate of the search. The smallest value \texttt{$\rho$} found within a tolerance of $10^{-5}$ is returned as the guaranteed convergence rate, and the analysis process is complete.